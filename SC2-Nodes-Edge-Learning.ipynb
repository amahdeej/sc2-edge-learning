{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import pathlib\n",
    "import sqlite3\n",
    "import tempfile\n",
    "import importlib\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sru import SRU, SRUCell\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "from configparser import ConfigParser\n",
    "from distutils.spawn import find_executable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import OrderedDict, namedtuple\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.cuda.set_device(2)\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(3, 50)\n",
    "\n",
    "        self.selu = nn.SELU()\n",
    "        self.drop = nn.AlphaDropout(p=0.5)\n",
    "        #self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(50, 50)\n",
    "        \n",
    "        self.linear3 = nn.Linear(50, 50)\n",
    "\n",
    "        self.linear4 = nn.Linear(50, 50)\n",
    "\n",
    "        self.linear5 = nn.Linear(50,2)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.linear1(input)\n",
    "        output = self.selu(output)\n",
    "        #output = self.relu(output)\n",
    "\n",
    "        output = self.linear2(output)\n",
    "        output = self.selu(output)\n",
    "        #output = self.relu(output)\n",
    "\n",
    "        output = self.linear3(output)\n",
    "        output = self.selu(output)  \n",
    "        #output = self.relu(output)\n",
    "\n",
    "        output = self.linear4(output)\n",
    "        output = self.selu(output)\n",
    "        #output = self.relu(output)\n",
    "\n",
    "        output = self.drop(output)\n",
    "        output = self.linear5(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBSDNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=16, kernel_size=3, padding=1)\n",
    "\n",
    "        self.linear1 = nn.Linear(16*3, 100)\n",
    "        \n",
    "        self.sru = SRU(input_size=100, hidden_size=100,num_layers=2,bidirectional='True')\n",
    "        self.linear2 = nn.Linear(100*2, 100*2)\n",
    "\n",
    "        self.linear3 = nn.Linear(100*2, 2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        self.dropout2 = nn.Dropout(0.4)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        output = self.conv1(input)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.dropout1(output)\n",
    "                \n",
    "        output = output.view(-1, 16*3)\n",
    "        output1 = self.linear1(output)\n",
    "\n",
    "        output = output1.view(len(output1),1 ,-1 )\n",
    "        output, _ = self.sru(output)\n",
    "        \n",
    "        output = output.view(len(output1), -1)\n",
    "\n",
    "        output = self.linear2(output)\n",
    "        output = self.relu(output)\n",
    "\n",
    "        output = self.dropout2(output)\n",
    "\n",
    "        output = self.linear3(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Scrimmage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scr = 4 # Scrimmage 4 or 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scr == 4:\n",
    "    with open('scrimmage4_edge_node_dataset.pickle', 'rb') as file:\n",
    "        link_data = pickle.load(file)\n",
    "        \n",
    "if scr == 5:\n",
    "    with open('scrimmage5_edge_node_dataset.pickle', 'rb') as file:\n",
    "        link_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = torch.LongTensor([0,1,3]) # Select Columns\n",
    "\n",
    "link_data = [(link_datas[0][:,cols], link_datas[1]) for link_datas in link_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats to convert noise variance to SNR\n",
    "scr4_mean = 31.827541\n",
    "scr4_std = 7.5468507\n",
    "\n",
    "scr5_mean = 33.17964\n",
    "scr5_std = 6.6672482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadist_edge = [0.4, 0.1, 0.50] # distribution among train, validation, \n",
    "\n",
    "test_x_npn = []\n",
    "test_y_npn = []\n",
    "edge_train_x_npn= []\n",
    "edge_train_y_npn = []\n",
    "edge_val_x_npn = []\n",
    "edge_val_y_npn = []\n",
    "\n",
    "for i in range(len(link_data)):\n",
    "    datalen = len(link_data[i][1])\n",
    "    edge_trainlen = int(datalen*sum(datadist_edge[:1]))\n",
    "    edge_vallen = int(datalen*sum(datadist_edge[:2]))\n",
    "\n",
    "    edge_train_x_npn.append(link_data[i][0][0:edge_trainlen].numpy())    \n",
    "    edge_train_y_npn.append(link_data[i][1][0:edge_trainlen].numpy())\n",
    "    \n",
    "    edge_val_x_npn.append(link_data[i][0][edge_trainlen:edge_vallen].numpy())    \n",
    "    edge_val_y_npn.append(link_data[i][1][edge_trainlen:edge_vallen].numpy())\n",
    "    \n",
    "    test_x_npn.append(link_data[i][0][edge_vallen:datalen].numpy())\n",
    "    test_y_npn.append(link_data[i][1][edge_vallen:datalen].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tensors for Batch Processing\n",
    "# Edge Training and Testing for individual links\n",
    "edge_train_x = [torch.from_numpy(link_x).type(torch.float) for link_x in edge_train_x_npn]\n",
    "edge_val_x = [torch.from_numpy(link_x).type(torch.float) for link_x in edge_val_x_npn]\n",
    "test_x = [torch.from_numpy(link_x).type(torch.float) for link_x in test_x_npn]\n",
    "\n",
    "edge_train_y = [torch.from_numpy(link_y).type(torch.long) for link_y in edge_train_y_npn]\n",
    "edge_val_y = [torch.from_numpy(link_y).type(torch.long) for link_y in edge_val_y_npn]\n",
    "test_y = [torch.from_numpy(link_y).type(torch.long) for link_y in test_y_npn]                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Local Training Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_val_loss = 100\n",
    "number_epoch_until_best = 1\n",
    "training_time = 0\n",
    "training_time_until_best = 0\n",
    "average_time_per_epoch = 0\n",
    "\n",
    "model = MLP().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs in edge training\n",
    "NUM_EPOCHS_EDGE = 100\n",
    "prd = []\n",
    "trg = []\n",
    "snr = []\n",
    "\n",
    "for i in range(len(link_data)):\n",
    "    \n",
    "    # Train\n",
    "    edge_train_dataloader=data.DataLoader(data.TensorDataset(edge_train_x[i],edge_train_y[i]),\n",
    "                                     batch_size=TRAIN_BATCH_SIZE, shuffle=True, \n",
    "                                     num_workers=16, pin_memory=True)\n",
    "\n",
    "    edge_val_dataloader=data.DataLoader(data.TensorDataset(edge_val_x[i],edge_val_y[i]),\n",
    "                                   batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                   num_workers=16, pin_memory=True)\n",
    "    start_time = time.time()\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = 100\n",
    "    number_epoch_until_best = 1\n",
    "    training_time = 0\n",
    "    training_time_until_best = 0\n",
    "    average_time_per_epoch = 0\n",
    "    #teacher_outputs = fetch_teacher_outputs(model1, train_dataloader)\n",
    "    for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "\n",
    "        progress_edge_training_epoch = tqdm(\n",
    "            edge_train_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "            miniters=1, ncols=88, position=0,\n",
    "            leave=True, total=len(edge_train_dataloader), smoothing=.9)\n",
    "\n",
    "        progress_edge_validation_epoch = tqdm(\n",
    "            edge_val_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(edge_val_dataloader), smoothing=.9)\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_size = 0\n",
    "        model.train()           \n",
    "        for idx, (sentence, tags) in enumerate(progress_edge_training_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * tags.size()[0]\n",
    "            train_size += tags.size()[0]\n",
    "                \n",
    "        test_loss = 0\n",
    "        test_size = 0    \n",
    "        test_total_num_correct = 0\n",
    "        predict = []\n",
    "        target = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_validation_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                loss = loss_function(tag_scores, tags)\n",
    "                predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target.append(tags.cpu().numpy())        \n",
    "                test_loss += loss * tags.size()[0]\n",
    "                test_size += tags.size()[0]\n",
    "                test_total_num_correct += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "        val_accuracy = test_total_num_correct.item()/test_size\n",
    "        val_loss = test_loss.item()/test_size\n",
    "        predict = np.concatenate(predict, axis=0)\n",
    "        target = np.concatenate(target, axis=0)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            number_epoch_until_best_accuracy = epoch_idx\n",
    "            training_time_until_best = training_time\n",
    "            torch.save(model.state_dict(), 'mlp_local.pt')\n",
    "\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "        print(f'epoch:{epoch_idx}, '\n",
    "              f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "              f'validation loss:{val_loss: .5f}, '\n",
    "              f'accuracy: {val_accuracy: .4f}, '\n",
    "              f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "        if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "            break\n",
    "\n",
    "    print(f'number of epochs: {number_epoch_until_best_accuracy}')\n",
    "    model.load_state_dict(torch.load('mlp_local.pt'))\n",
    "    model.cuda()\n",
    "    \n",
    "    # Test\n",
    "    test_dataloader=data.DataLoader(data.TensorDataset(test_x[i],test_y[i]), \n",
    "                                       batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                                       num_workers=16, pin_memory=True)\n",
    "\n",
    "    progress_test_epoch = tqdm(\n",
    "        test_dataloader, \n",
    "        desc=f'Link {i}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    snrss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "            snrss.append(sentence.cpu().numpy()[:,0])\n",
    "\n",
    "    prd.append(np.concatenate(predict, axis=0))\n",
    "    trg.append(np.concatenate(target, axis=0))\n",
    "    snr.append(np.concatenate(snrss, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:\n",
    "    mlp_scr4_edge_res = {}\n",
    "\n",
    "    mlp_scr4_edge_res['mlp_scr4_prd'] = prd\n",
    "    mlp_scr4_edge_res['mlp_scr4_trg'] = trg\n",
    "    mlp_scr4_edge_res['mlp_scr4_snr'] = [snrs * scr4_std + scr4_mean for snrs in snr]\n",
    "\n",
    "    #Name of pickle file to be saved\n",
    "    outname = 'mlp_scr4_nodes_local_res.pickle'                    \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr4_edge_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 5:\n",
    "    mlp_scr5_edge_res = {}\n",
    "\n",
    "    mlp_scr5_edge_res['mlp_scr5_prd'] = prd\n",
    "    mlp_scr5_edge_res['mlp_scr5_trg'] = trg\n",
    "    mlp_scr5_edge_res['mlp_scr5_snr'] = [snrs * scr5_std + scr5_mean for snrs in snr]\n",
    "\n",
    "    #Name of pickle file to be saved\n",
    "    outname = 'mlp_scr5_nodes_local_res.pickle'\n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr5_edge_res, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. FedAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weights(w):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for key in w_avg.keys():\n",
    "        for i in range(1, len(w)):\n",
    "            w_avg[key] += w[i][key]\n",
    "        w_avg[key] = torch.div(w_avg[key], len(w))\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_links = len(link_data)\n",
    "n_round = 10 # Number of Rounds\n",
    "n_repeat = 10 # Maximum how many times a link can be repeated\n",
    "n_links = int(num_total_links/(n_repeat+1)) # No of links per round\n",
    "\n",
    "# Initialize matrix with out of bound value as the count is used to limit no of occurances\n",
    "\n",
    "link_occurance_count = np.zeros(num_total_links)\n",
    "randlist = num_total_links * np.ones((n_round,n_links))\n",
    "link_list = np.arange(0,num_total_links)\n",
    "\n",
    "\n",
    "for i in range(0,n_round):  \n",
    "    \n",
    "    mask = link_occurance_count < n_repeat\n",
    "    # only select links that has not yet reached maximum occurance\n",
    "    newlinks = np.random.choice(link_list[mask], size=n_links, replace=False) \n",
    "    \n",
    "    randlist[i] = newlinks\n",
    "    \n",
    "    for j in newlinks:\n",
    "        link_occurance_count[j] += 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize global model\n",
    "global_model = MLP().cuda()\n",
    "\n",
    "# Number of epochs in edge training\n",
    "NUM_EPOCHS = 10\n",
    "NUM_EPOCHS_EDGE = 10\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "\n",
    "state_dicts = []\n",
    "global_weights = []\n",
    "for comm_round in range(n_round):\n",
    "    \n",
    "    selected_edges = randlist[comm_round].astype(int)\n",
    "\n",
    "    local_weights = []\n",
    "    states = []\n",
    "    \n",
    "    for count,i in enumerate(selected_edges): # Train on each edge\n",
    "        print('Round :', comm_round+1, ' of ', n_round,', Link :', count+1, ' of ', n_links)\n",
    "        \n",
    "        model = global_model\n",
    "        model.cuda()\n",
    "\n",
    "        loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "\n",
    "        optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)\n",
    "        \n",
    "        edge_train_dataloader=data.DataLoader(data.TensorDataset(edge_train_x[i],edge_train_y[i]),\n",
    "                                         batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                                         num_workers=16, pin_memory=True)\n",
    "\n",
    "        edge_val_dataloader=data.DataLoader(data.TensorDataset(edge_val_x[i],edge_val_y[i]),\n",
    "                                       batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                       num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_val_accuracy = 0\n",
    "        best_val_loss = 100\n",
    "        number_epoch_until_best = 1\n",
    "        training_time = 0\n",
    "        training_time_until_best = 0\n",
    "        average_time_per_epoch = 0        \n",
    "        for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "\n",
    "            progress_edge_training_epoch = tqdm(\n",
    "                edge_train_dataloader, \n",
    "                desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "                miniters=1, ncols=88, position=0,\n",
    "                leave=True, total=len(edge_train_dataloader), smoothing=.9, disable = True)\n",
    "\n",
    "            progress_edge_validation_epoch = tqdm(\n",
    "                edge_val_dataloader, \n",
    "                desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "                miniters=1, ncols=88, position=0, \n",
    "                leave=True, total=len(edge_val_dataloader), smoothing=.9, disable = True)\n",
    "\n",
    "            train_loss = 0\n",
    "            train_size = 0\n",
    "            model.train() \n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_training_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                tag_scores = model(sentence)\n",
    "                loss = loss_function(tag_scores, tags)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss * tags.size()[0]\n",
    "                train_size += tags.size()[0]\n",
    "\n",
    "            val_loss = 0\n",
    "            val_size = 0\n",
    "            val_total_num_correct = 0\n",
    "            predict = []\n",
    "            target = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for idx, (sentence, tags) in enumerate(progress_edge_validation_epoch):\n",
    "                    sentence = sentence.cuda()\n",
    "                    tags = tags.cuda()\n",
    "                    tag_scores = model(sentence)\n",
    "                    loss = loss_function(tag_scores, tags)\n",
    "                    predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                    target.append(tags.cpu().numpy())        \n",
    "                    val_loss += loss * tags.size()[0]\n",
    "                    val_size += tags.size()[0]\n",
    "                    val_total_num_correct += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "\n",
    "            val_accuracy = val_total_num_correct.item()/val_size\n",
    "            val_loss = val_loss.item()/val_size\n",
    "            predict = np.concatenate(predict, axis=0)\n",
    "            target = np.concatenate(target, axis=0)\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                number_epoch_until_best_accuracy = epoch_idx\n",
    "                training_time_until_best = training_time\n",
    "                # Save Model\n",
    "                state = {'state_dict': model.state_dict()}\n",
    "                if Scr == 4:\n",
    "                    torch.save(model.state_dict(),'mlp_fd_scr4.pt')\n",
    "                    torch.save(state,'mlp_fd_scr4.pth')\n",
    "                if Scr == 5:\n",
    "                    torch.save(model.state_dict(),'mlp_fd_scr5.pt')\n",
    "                    torch.save(state,'mlp_fd_scr5.pth')\n",
    "                    \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "            print(f'epoch:{epoch_idx}, '\n",
    "                  f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "                  f'validation loss:{val_loss: .5f}, '\n",
    "                  f'accuracy: {val_accuracy: .4f}, '\n",
    "                  f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "            if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "                break\n",
    "\n",
    "        if Scr == 4:     \n",
    "            local_weights.append(copy.deepcopy(torch.load('mlp_fd_scr4.pt')))            \n",
    "            states.append(torch.load('mlp_fd_scr4.pth'))\n",
    "        if Scr == 5:     \n",
    "            local_weights.append(copy.deepcopy(torch.load('mlp_fd_scr5.pt')))            \n",
    "            states.append(torch.load('mlp_fd_scr5.pth'))\n",
    "            \n",
    "    global_weight = average_weights(local_weights)           \n",
    "    global_weights.append(average_weights(local_weights))\n",
    "    \n",
    "    # Edge based weighting\n",
    "    num_edges = n_links\n",
    "    weights_1 = np.ones(num_edges)/num_edges\n",
    "\n",
    "    # weight the state dicts\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        for key in states[i]['state_dict'].keys():\n",
    "            states[i]['state_dict'][key] *= weights_1[i]\n",
    "\n",
    "    state_dict_fed = states[0]['state_dict'].copy() # initialize the dict with weighted first edge, then add others\n",
    "\n",
    "    for i in range(1,len(states)):\n",
    "        for key in states[i]['state_dict'].keys():\n",
    "            state_dict_fed[key] = state_dict_fed[key] + states[i]['state_dict'][key]\n",
    "    \n",
    "    # Save comm round state dicts\n",
    "    state_dicts.append(state_dict_fed)\n",
    "    global_model.load_state_dict(global_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4: \n",
    "    outname = 'fed_multiround_scr4_states.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(state_dicts, file)\n",
    "\n",
    "    outname = 'fed_multiround_scr4_weights.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(global_weights, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 5: \n",
    "    outname = 'fed_multiround_scr5_states.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(state_dicts, file)\n",
    "\n",
    "    outname = 'fed_multiround_scr5_weights.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(global_weights, file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test on each edge using federated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4: \n",
    "    with open('fed_multiround_scr4_states.pickle', 'rb') as file:\n",
    "        state_dicts = pickle.load(file)\n",
    "        \n",
    "if Scr == 5: \n",
    "    with open('fed_multiround_scr5_states.pickle', 'rb') as file:\n",
    "        state_dicts = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 64\n",
    "\n",
    "prds = []\n",
    "trgs = []\n",
    "snrs = []\n",
    "\n",
    "\n",
    "for j in range(n_round):\n",
    "\n",
    "    model = global_model\n",
    "    model.load_state_dict(state_dicts[j]) \n",
    "    model.cuda()  \n",
    "\n",
    "    prd = []\n",
    "    trg = []\n",
    "    snr = []\n",
    "\n",
    "    for i in range(num_total_links): # Train on each edge   \n",
    "        print('Round :', j+1, ' of ', n_round,', Link :', i+1, ' of ', num_total_links)\n",
    "        \n",
    "        test_dataloader=data.DataLoader(data.TensorDataset(test_x[i],test_y[i]), \n",
    "                                           batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
    "                                           num_workers=16, pin_memory=True)\n",
    "\n",
    "        progress_test_epoch = tqdm(\n",
    "            test_dataloader, \n",
    "            desc=f'Link {i}, test',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(test_dataloader), smoothing=.9,disable=True)\n",
    "\n",
    "        predict = []\n",
    "        target = []\n",
    "        snrss = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target.append(tags.cpu().numpy())\n",
    "                snrss.append(sentence.cpu().numpy()[:,0])\n",
    "\n",
    "        prd.append(np.concatenate(predict, axis=0))\n",
    "        trg.append(np.concatenate(target, axis=0))\n",
    "        snr.append(np.concatenate(snrss, axis=0))\n",
    "        \n",
    "    prds.append([prd])\n",
    "    trgs.append([trg])\n",
    "    if Scr == 4:\n",
    "        snrs.append([[snrs * scr4_std + scr4_mean for snrs in snr]])\n",
    "    if Scr == 5:\n",
    "        snrs.append([[snrs * scr5_std + scr5_mean for snrs in snr]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4: \n",
    "    mlp_scr4_edge_fed_res_multiround = {}\n",
    "\n",
    "    mlp_scr4_edge_fed_res_multiround['mlp_scr4_prd'] = prds\n",
    "    mlp_scr4_edge_fed_res_multiround['mlp_scr4_trg'] = trgs\n",
    "    mlp_scr4_edge_fed_res_multiround['mlp_scr4_snr'] = snrs\n",
    "\n",
    "    outname = 'mlp_scr4_edge_fed_res_multiround.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr4_edge_fed_res_multiround, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 5: \n",
    "    mlp_scr5_edge_fed_res_multiround = {}\n",
    "\n",
    "    mlp_scr5_edge_fed_res_multiround['mlp_scr5_prd'] = prds\n",
    "    mlp_scr5_edge_fed_res_multiround['mlp_scr5_trg'] = trgs\n",
    "    mlp_scr5_edge_fed_res_multiround['mlp_scr5_snr'] = snrs\n",
    "\n",
    "    outname = 'mlp_scr5_edge_fed_res_multiround.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr5_edge_fed_res_multiround, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DP-Fed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_params(self, state_dict, coefficient_transfer=100):\n",
    "    own_state = self.state_dict()\n",
    "    for name, param in state_dict.items():\n",
    "        if name in own_state:\n",
    "            own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Train on Edges and Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_links = len(link_data)\n",
    "n_round = 10 # Number of Rounds\n",
    "n_repeat = 10 # Maximum how many times a link can be repeated\n",
    "n_links = int(num_total_links/(n_repeat+1)) # No of links per round\n",
    "\n",
    "# Initialize matrix with out of bound value as the count is used to limit no of occurances\n",
    "\n",
    "link_occurance_count = np.zeros(num_total_links)\n",
    "randlist = num_total_links * np.ones((n_round,n_links))\n",
    "link_list = np.arange(0,num_total_links)\n",
    "\n",
    "\n",
    "for i in range(0,n_round):  \n",
    "    \n",
    "    mask = link_occurance_count < n_repeat\n",
    "    # only select links that has not yet reached maximum occurance\n",
    "    newlinks = np.random.choice(link_list[mask], size=n_links, replace=False) \n",
    "    \n",
    "    randlist[i] = newlinks\n",
    "    \n",
    "    for j in newlinks:\n",
    "        link_occurance_count[j] += 1                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize global model\n",
    "global_model = MLP().cuda()\n",
    "\n",
    "\n",
    "# Number of epochs in edge training\n",
    "NUM_EPOCHS = 10\n",
    "NUM_EPOCHS_EDGE = 10\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "MAX_GRAD_NORM = 1.2\n",
    "NOISE_MULTIPLIER = 1\n",
    "sigma = 0.01\n",
    "state_dicts = []\n",
    "global_weights = []\n",
    "for comm_round in range(n_round):\n",
    "    \n",
    "    selected_edges = randlist[comm_round].astype(int)\n",
    "\n",
    "    local_weights = []\n",
    "    states = []\n",
    "\n",
    "    for count,i in enumerate(selected_edges): # Train on each edge\n",
    "        print('Round :', comm_round+1, ' of ', n_round,', Link :', count+1, ' of ', n_links)\n",
    "        \n",
    "        model = MLP().cuda()\n",
    "        model.load_state_dict(copy.deepcopy(global_model.state_dict()))\n",
    "        optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)\n",
    "        privacy_engine = PrivacyEngine(\n",
    "        model,\n",
    "        batch_size=TRAIN_BATCH_SIZE,\n",
    "        sample_size=len(edge_train_x[i]),\n",
    "        alphas=range(10,100),\n",
    "        noise_multiplier=NOISE_MULTIPLIER,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        )\n",
    "        privacy_engine.attach(optimizer)\n",
    "        loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "\n",
    "\n",
    "        edge_train_dataloader=data.DataLoader(data.TensorDataset(edge_train_x[i],edge_train_y[i]),\n",
    "                                         batch_size=TRAIN_BATCH_SIZE, shuffle=False, \n",
    "                                         num_workers=16, pin_memory=True)\n",
    "\n",
    "        edge_val_dataloader=data.DataLoader(data.TensorDataset(edge_val_x[i],edge_val_y[i]),\n",
    "                                       batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                       num_workers=16, pin_memory=True)\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        best_val_accuracy = 0\n",
    "        best_val_loss = 100\n",
    "        number_epoch_until_best = 1\n",
    "        training_time = 0\n",
    "        training_time_until_best = 0\n",
    "        average_time_per_epoch = 0        \n",
    "        for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "\n",
    "            progress_edge_training_epoch = tqdm(\n",
    "                edge_train_dataloader, \n",
    "                desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "                miniters=1, ncols=88, position=0,\n",
    "                leave=True, total=len(edge_train_dataloader), smoothing=.9, disable = True)\n",
    "\n",
    "            progress_edge_validation_epoch = tqdm(\n",
    "                edge_val_dataloader, \n",
    "                desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "                miniters=1, ncols=88, position=0, \n",
    "                leave=True, total=len(edge_val_dataloader), smoothing=.9, disable = True)\n",
    "\n",
    "            train_loss = 0\n",
    "            train_size = 0\n",
    "            model.train() \n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_training_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                optimizer.zero_grad()\n",
    "                tag_scores = model(sentence)\n",
    "                loss =loss_function(tag_scores,tags)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                train_loss += loss * tags.size()[0]\n",
    "                train_size += tags.size()[0]\n",
    "\n",
    "            val_loss = 0\n",
    "            val_size = 0\n",
    "            val_total_num_correct = 0\n",
    "            predict = []\n",
    "            target = []\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for idx, (sentence, tags) in enumerate(progress_edge_validation_epoch):\n",
    "                    sentence = sentence.cuda()\n",
    "                    tags = tags.cuda()\n",
    "                    tag_scores = model(sentence)\n",
    "                    loss =loss_function(tag_scores,tags)\n",
    "                    predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                    target.append(tags.cpu().numpy())        \n",
    "                    val_loss += loss * tags.size()[0]\n",
    "                    val_size += tags.size()[0]\n",
    "                    val_total_num_correct += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "\n",
    "            val_accuracy = val_total_num_correct.item()/val_size\n",
    "            val_loss = val_loss.item()/val_size\n",
    "            predict = np.concatenate(predict, axis=0)\n",
    "            target = np.concatenate(target, axis=0)\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                number_epoch_until_best_accuracy = epoch_idx\n",
    "                training_time_until_best = training_time\n",
    "                #torch.save(model.state_dict(), 'mlp_fd.pt')\n",
    "                # Save Model\n",
    "                state = {'state_dict': model.state_dict()}\n",
    "                if Scr == 4:\n",
    "                    torch.save(model.state_dict(),'mlp_fdd_scr4.pt')\n",
    "                    torch.save(state,'mlp_fdd_scr4.pth')\n",
    "                if Scr == 5:\n",
    "                    torch.save(model.state_dict(),'mlp_fdd_scr5.pt')\n",
    "                    torch.save(state,'mlp_fdd_scr5.pth')\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "            print(f'epoch:{epoch_idx}, '\n",
    "                  f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "                  f'validation loss:{val_loss: .5f}, '\n",
    "                  f'accuracy: {val_accuracy: .4f}, '\n",
    "                  f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "            if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "                break\n",
    "\n",
    "             \n",
    "        if Scr == 4:     \n",
    "            local_weights.append(copy.deepcopy(torch.load('mlp_fdd_scr4.pt')))            \n",
    "            states.append(torch.load('mlp_fdd_scr4.pth'))\n",
    "        if Scr == 5:     \n",
    "            local_weights.append(copy.deepcopy(torch.load('mlp_fdd_scr5.pt')))            \n",
    "            states.append(torch.load('mlp_fdd_scr5.pth'))\n",
    "    \n",
    "    global_weight = average_weights(local_weights)           \n",
    "    global_weights.append(average_weights(local_weights))\n",
    "    \n",
    "    # Edge based weighting\n",
    "    num_edges = n_links\n",
    "    weights_1 = np.ones(num_edges)/num_edges\n",
    "\n",
    "    # weight the state dicts\n",
    "\n",
    "    for i in range(len(states)):\n",
    "        for key in states[i]['state_dict'].keys():\n",
    "            states[i]['state_dict'][key] *= weights_1[i]\n",
    "\n",
    "    state_dict_fed = states[0]['state_dict'].copy() # initialize the dict with weighted first edge, then add others\n",
    "\n",
    "    for i in range(1,len(states)):\n",
    "        for key in states[i]['state_dict'].keys():\n",
    "            state_dict_fed[key] = state_dict_fed[key] + states[i]['state_dict'][key]\n",
    "    \n",
    "    # Save comm round state dicts\n",
    "    state_dicts.append(state_dict_fed)\n",
    "    global_model.load_state_dict(global_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4: \n",
    "    outname = 'fedd_multiround_scr4_states.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(state_dicts, file)\n",
    "\n",
    "    outname = 'fedd_multiround_scr4_weights.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(global_weights, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 5: \n",
    "    outname = 'fedd_multiround_scr5_states.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(state_dicts, file)\n",
    "\n",
    "    outname = 'fedd_multiround_scr5_weights.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(global_weights, file)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Test on each edge using federated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:\n",
    "    with open('fedd_multiround_scr4_states.pickle', 'rb') as file:\n",
    "        state_dicts = pickle.load(file)\n",
    "\n",
    "if Scr == 5:\n",
    "    with open('fedd_multiround_scr5_states.pickle', 'rb') as file:\n",
    "        state_dicts = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BATCH_SIZE = 64\n",
    "\n",
    "prds = []\n",
    "trgs = []\n",
    "snrs = []\n",
    "\n",
    "\n",
    "for j in range(n_round):\n",
    "\n",
    "    model = global_model\n",
    "    model.load_state_dict(state_dicts[j]) \n",
    "    model.cuda()  \n",
    "\n",
    "    prd = []\n",
    "    trg = []\n",
    "    snr = []\n",
    "\n",
    "    for i in range(num_total_links): # Train on each edge   \n",
    "        print('Round :', j+1, ' of ', n_round,', Link :', i+1, ' of ', num_total_links)\n",
    "        \n",
    "        test_dataloader=data.DataLoader(data.TensorDataset(test_x[i],test_y[i]), \n",
    "                                           batch_size=TEST_BATCH_SIZE, shuffle=False,\n",
    "                                           num_workers=16, pin_memory=True)\n",
    "\n",
    "        progress_test_epoch = tqdm(\n",
    "            test_dataloader, \n",
    "            desc=f'Link {i}, test',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(test_dataloader), smoothing=.9,disable=True)\n",
    "\n",
    "        predict = []\n",
    "        target = []\n",
    "        snrss = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target.append(tags.cpu().numpy())\n",
    "                snrss.append(sentence.cpu().numpy()[:,0])\n",
    "\n",
    "        prd.append(np.concatenate(predict, axis=0))\n",
    "        trg.append(np.concatenate(target, axis=0))\n",
    "        snr.append(np.concatenate(snrss, axis=0))\n",
    "        \n",
    "    prds.append([prd])\n",
    "    trgs.append([trg])\n",
    "    if Scr == 4:\n",
    "        snrs.append([[snrs * scr4_std + scr4_mean for snrs in snr]])\n",
    "    if Scr == 5:\n",
    "        snrs.append([[snrs * scr5_std + scr5_mean for snrs in snr]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:\n",
    "\n",
    "    mlp_scr4_edge_fed_res_multiround = {}\n",
    "\n",
    "    mlp_scr4_edge_fed_res_multiround['mlp_scr4_prd'] = prds\n",
    "    mlp_scr4_edge_fed_res_multiround['mlp_scr4_trg'] = trgs\n",
    "    mlp_scr4_edge_fed_res_multiround['mlp_scr4_snr'] = snrs\n",
    "\n",
    "    outname = 'mlp_scr4_edge_dpfed_res_multiround.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr4_edge_fed_res_multiround, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 5:\n",
    "\n",
    "    mlp_scr5_edge_fed_res_multiround = {}\n",
    "\n",
    "    mlp_scr5_edge_fed_res_multiround['mlp_scr5_prd'] = prds\n",
    "    mlp_scr5_edge_fed_res_multiround['mlp_scr5_trg'] = trgs\n",
    "    mlp_scr5_edge_fed_res_multiround['mlp_scr5_snr'] = snrs\n",
    "\n",
    "    outname = 'mlp_scr5_edge_dpfed_res_multiround.pickle'         \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr5_edge_fed_res_multiround, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. KD-Scr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kd loss func\n",
    "\n",
    "def loss_kd(outputs, labels, teacher_outputs, alpha, T):\n",
    "    \"\"\"\n",
    "    loss function for Knowledge Distillation (KD)\n",
    "    \"\"\"\n",
    "    loss_CE = F.cross_entropy(outputs, labels)\n",
    "    D_KL = F.kl_div(F.log_softmax(outputs/T, dim=1),F.softmax(teacher_outputs/T, dim=1),reduction='batchmean') * (T * T)\n",
    "    KD_loss =  (1. - alpha)*loss_CE + alpha*D_KL\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Teacher CBSDNN training on Original Scrimmage data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Training for all links - convert scr edge train val to central\n",
    "train_x = torch.cat(tuple(torch.from_numpy(link_x).type(torch.float) \n",
    "                          for link_x in edge_train_x_npn),dim=0)\n",
    "val_x = torch.cat(tuple(torch.from_numpy(link_x).type(torch.float) \n",
    "                          for link_x in edge_val_x_npn),dim=0)\n",
    "\n",
    "\n",
    "train_y = torch.cat(tuple(torch.from_numpy(link_y).type(torch.long) \n",
    "                          for link_y in edge_train_y_npn),dim=0)\n",
    "val_y = torch.cat(tuple(torch.from_numpy(link_y).type(torch.long) \n",
    "                        for link_y in edge_val_y_npn),dim=0)\n",
    "train_x = train_x.view(-1,1,3)\n",
    "val_x = val_x.view(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 1024\n",
    "VAL_BATCH_SIZE = 128\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_val_loss = 100\n",
    "number_epoch_until_best = 1\n",
    "training_time = 0\n",
    "training_time_until_best = 0\n",
    "average_time_per_epoch = 0\n",
    "\n",
    "model = CBSDNN().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training in Central Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS):  \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "\n",
    "    for idx, (input, target) in enumerate(progress_training_epoch):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        model.zero_grad()\n",
    "        predict = model(input)\n",
    "        loss = loss_function(predict, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * target.size()[0]\n",
    "        train_size += target.size()[0]\n",
    "    \n",
    "    training_time += time.time() - start_time\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    test_total_num_correct = 0\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_validation_epoch):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            loss = loss_function(batch_predict, batch_target)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "            test_loss += loss * batch_target.size()[0]\n",
    "            test_size += batch_target.size()[0]\n",
    "            test_total_num_correct += torch.eq(batch_predict.argmax(dim=1), batch_target).sum()  \n",
    "    val_accuracy = test_total_num_correct.item()/test_size\n",
    "    val_loss = test_loss.item()/test_size\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        number_epoch_until_best_accuracy = epoch_idx\n",
    "        training_time_until_best = training_time\n",
    "        # Save Model\n",
    "        if Scr == 4:\n",
    "            torch.save(model.state_dict(), 'cbsdnn_scr4_nodes.pt')        \n",
    "            state = {\n",
    "            'epoch': NUM_EPOCHS,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            savepath='cbsdnn_scr4_nodes_checkpoint.pth'\n",
    "            torch.save(state,savepath)            \n",
    "        if Scr == 5:\n",
    "            torch.save(model.state_dict(), 'cbsdnn_scr5_nodes.pt')        \n",
    "            state = {\n",
    "            'epoch': NUM_EPOCHS,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            savepath='cbsdnn_scr5_nodes_checkpoint.pth'\n",
    "            torch.save(state,savepath)\n",
    "            \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "    print(f'epoch:{epoch_idx}, '\n",
    "          f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{val_loss: .5f}, '\n",
    "          f'accuracy: {val_accuracy: .4f}, '\n",
    "          f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "    if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'total training time: {training_time_until_best}')\n",
    "print(f'number of epochs: {number_epoch_until_best_accuracy}')\n",
    "print(f'time per epoch: {(training_time_until_best/number_epoch_until_best_accuracy): .2f}')    \n",
    "    \n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Loading Model, Further Link Specific Training and Edge Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:\n",
    "    checkpoint = torch.load('cbsdnn_scr4_nodes_checkpoint.pth')\n",
    "if Scr == 5:\n",
    "    checkpoint = torch.load('cbsdnn_scr5_nodes_checkpoint.pth')\n",
    "\n",
    "model1.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs in edge training\n",
    "NUM_EPOCHS_EDGE = 100.\n",
    "prd = []\n",
    "trg = []\n",
    "snr = []\n",
    "\n",
    "# Parameters for kd loss func\n",
    "if Scr == 4:\n",
    "    alpha = 0.4\n",
    "    T = 10.0\n",
    "if Scr == 5:\n",
    "    alpha = 0.5\n",
    "    T = 10.0\n",
    "    \n",
    "for i in range(len(link_data)):\n",
    "    \n",
    "    # Train\n",
    "    edge_train_dataloader=data.DataLoader(data.TensorDataset(edge_train_x[i],edge_train_y[i]),\n",
    "                                     batch_size=TRAIN_BATCH_SIZE, shuffle=True, \n",
    "                                     num_workers=16, pin_memory=True)\n",
    "\n",
    "    edge_val_dataloader=data.DataLoader(data.TensorDataset(edge_val_x[i],edge_val_y[i]),\n",
    "                                   batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                   num_workers=16, pin_memory=True)\n",
    "    start_time = time.time()\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = 100\n",
    "    number_epoch_until_best = 1\n",
    "    training_time = 0\n",
    "    training_time_until_best = 0\n",
    "    average_time_per_epoch = 0\n",
    "    for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "\n",
    "        progress_edge_training_epoch = tqdm(\n",
    "            edge_train_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "            miniters=1, ncols=88, position=0,\n",
    "            leave=True, total=len(edge_train_dataloader), smoothing=.9,disable=True)\n",
    "\n",
    "        progress_edge_validation_epoch = tqdm(\n",
    "            edge_val_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(edge_val_dataloader), smoothing=.9,disable=True)\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_size = 0\n",
    "        model1.eval()\n",
    "        model.train()           \n",
    "        for idx, (sentence, tags) in enumerate(progress_edge_training_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            tag_scores = model(sentence)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = model1(sentence.view(-1,1,3))\n",
    "            loss =loss_kd(tag_scores,tags,teacher_outputs,alpha,T)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * tags.size()[0]\n",
    "            train_size += tags.size()[0]\n",
    "                \n",
    "        test_loss = 0\n",
    "        test_size = 0    \n",
    "        test_total_num_correct = 0\n",
    "        predict = []\n",
    "        target = []\n",
    "        model.eval()\n",
    "        model1.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_validation_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                teacher_outputs = model1(sentence.view(-1,1,3))\n",
    "                loss =loss_kd(tag_scores,tags,teacher_outputs,alpha,T)\n",
    "                predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target.append(tags.cpu().numpy())        \n",
    "                test_loss += loss * tags.size()[0]\n",
    "                test_size += tags.size()[0]\n",
    "                test_total_num_correct += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "        val_accuracy = test_total_num_correct.item()/test_size\n",
    "        val_loss = test_loss.item()/test_size\n",
    "        predict = np.concatenate(predict, axis=0)\n",
    "        target = np.concatenate(target, axis=0)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            number_epoch_until_best_accuracy = epoch_idx\n",
    "            training_time_until_best = training_time\n",
    "            if Scr == 4:\n",
    "                torch.save(model.state_dict(), 'mlp_scr4_kd_nodes.pt')\n",
    "            if Scr == 5:\n",
    "                torch.save(model.state_dict(), 'mlp_scr5_kd_nodes.pt')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "        print(f'epoch:{epoch_idx}, '\n",
    "              f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "              f'validation loss:{val_loss: .5f}, '\n",
    "              f'accuracy: {val_accuracy: .4f}, '\n",
    "              f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "        if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "            break\n",
    "\n",
    "    print(f'number of epochs: {number_epoch_until_best_accuracy}')\n",
    "    if Scr == 4:\n",
    "        model.load_state_dict(torch.load('mlp_scr4_kd_nodes.pt'))\n",
    "    if Scr == 5:\n",
    "        model.load_state_dict(torch.load('mlp_scr5_kd_nodes.pt'))\n",
    "    model.cuda()\n",
    "    \n",
    "    # Test\n",
    "\n",
    "    test_dataloader=data.DataLoader(data.TensorDataset(test_x[i],test_y[i]), \n",
    "                                       batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                                       num_workers=16, pin_memory=True)\n",
    "\n",
    "    progress_test_epoch = tqdm(\n",
    "        test_dataloader, \n",
    "        desc=f'Link {i}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    snrss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "            snrss.append(sentence.cpu().numpy()[:,0])\n",
    "\n",
    "    prd.append(np.concatenate(predict, axis=0))\n",
    "    trg.append(np.concatenate(target, axis=0))\n",
    "    snr.append(np.concatenate(snrss, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr ==4:\n",
    "    mlp_scr4_edge_res = {}\n",
    "\n",
    "    mlp_scr4_edge_res['mlp_scr4_prd'] = prd\n",
    "    mlp_scr4_edge_res['mlp_scr4_trg'] = trg\n",
    "    mlp_scr4_edge_res['mlp_scr4_snr'] = [snrs * scr4_std + scr4_mean for snrs in snr]\n",
    "\n",
    "    outname = 'mlp_scr4_nodes_kd_scr_res.pickle'                 \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr4_edge_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr ==5:\n",
    "    mlp_scr5_edge_res = {}\n",
    "\n",
    "    mlp_scr5_edge_res['mlp_scr5_prd'] = prd\n",
    "    mlp_scr5_edge_res['mlp_scr5_trg'] = trg\n",
    "    mlp_scr5_edge_res['mlp_scr5_snr'] = [snrs * scr5_std + scr5_mean for snrs in snr]\n",
    "\n",
    "    outname = 'mlp_scr5_nodes_kd_scr_res.pickle'                 \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr5_edge_res, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. KD-Smote"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Teacher CBSDNN training on SMOTE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "if Scr == 4:\n",
    "    with open('scr4_edge_smote_nodes_train.pickle', 'rb') as file:\n",
    "        link_data_smote = pickle.load(file)\n",
    "if Scr == 4:\n",
    "    with open('scr5_edge_smote_nodes_train.pickle', 'rb') as file:\n",
    "        link_data_smote = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = torch.LongTensor([0,1,3]) # Select Columns\n",
    "\n",
    "link_data_smote = [(link_datas[0][:,cols], link_datas[1]) for link_datas in link_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide into Train, Edge Train and Test Sets (With Validation)\n",
    "\n",
    "datadist = [0.8,0.2]                        \n",
    "\n",
    "train_x_npn = []\n",
    "train_y_npn = []\n",
    "val_x_npn = []\n",
    "val_y_npn = []\n",
    "\n",
    "for i in range(len(link_data_smote)):\n",
    "    datalen = len(link_data_smote[i][1])\n",
    "    trainlen = int(datalen*sum(datadist[:1]))\n",
    "    vallen = int(datalen*sum(datadist[:2]))\n",
    "\n",
    "    train_x_npn.append(link_data_smote[i][0][0:trainlen].numpy())    \n",
    "    train_y_npn.append(link_data_smote[i][1][0:trainlen].numpy())\n",
    "    \n",
    "    val_x_npn.append(link_data_smote[i][0][trainlen:vallen].numpy())    \n",
    "    val_y_npn.append(link_data_smote[i][1][trainlen:vallen].numpy())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Training for all links\n",
    "train_x = torch.cat(tuple(torch.from_numpy(link_x).type(torch.float) \n",
    "                          for link_x in train_x_npn),dim=0)\n",
    "val_x = torch.cat(tuple(torch.from_numpy(link_x).type(torch.float) \n",
    "                          for link_x in val_x_npn),dim=0)\n",
    "\n",
    "\n",
    "train_y = torch.cat(tuple(torch.from_numpy(link_y).type(torch.long) \n",
    "                          for link_y in train_y_npn),dim=0)\n",
    "val_y = torch.cat(tuple(torch.from_numpy(link_y).type(torch.long) \n",
    "                        for link_y in val_y_npn),dim=0)\n",
    "train_x = train_x.view(-1,1,3)\n",
    "val_x = val_x.view(-1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 1024\n",
    "VAL_BATCH_SIZE = 128\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_val_loss = 100\n",
    "number_epoch_until_best = 1\n",
    "training_time = 0\n",
    "training_time_until_best = 0\n",
    "average_time_per_epoch = 0\n",
    "\n",
    "model = CBSDNN().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Training in Central Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dataloader=data.DataLoader(data.TensorDataset(train_x,train_y),batch_size=TRAIN_BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=16, pin_memory=True)\n",
    "\n",
    "val_dataloader=data.DataLoader(data.TensorDataset(val_x,val_y),batch_size=VAL_BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "start_time = time.time()\n",
    "model.train()\n",
    "\n",
    "for epoch_idx in range(NUM_EPOCHS):  \n",
    "\n",
    "    progress_training_epoch = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Training',\n",
    "        miniters=1, ncols=88, position=0,\n",
    "        leave=True, total=len(train_dataloader), smoothing=.9)\n",
    "\n",
    "    progress_validation_epoch = tqdm(\n",
    "        val_dataloader, \n",
    "        desc=f'Epoch {epoch_idx}/{NUM_EPOCHS}, Validation',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(val_dataloader), smoothing=.9)\n",
    "\n",
    "    train_loss = 0\n",
    "    train_size = 0\n",
    "\n",
    "    for idx, (input, target) in enumerate(progress_training_epoch):\n",
    "        input = input.cuda()\n",
    "        target = target.cuda()\n",
    "        model.zero_grad()\n",
    "        predict = model(input)\n",
    "        loss = loss_function(predict, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss * target.size()[0]\n",
    "        train_size += target.size()[0]\n",
    "    \n",
    "    training_time += time.time() - start_time\n",
    "\n",
    "    test_loss = 0\n",
    "    test_size = 0\n",
    "    test_total_num_correct = 0\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (batch_input, batch_target) in enumerate(progress_validation_epoch):\n",
    "            batch_input = batch_input.cuda()\n",
    "            batch_target = batch_target.cuda()\n",
    "            batch_predict = model(batch_input)\n",
    "            loss = loss_function(batch_predict, batch_target)\n",
    "            predict.append(batch_predict.argmax(dim=1).cpu().numpy())\n",
    "            target.append(batch_target.cpu().numpy())        \n",
    "            test_loss += loss * batch_target.size()[0]\n",
    "            test_size += batch_target.size()[0]\n",
    "            test_total_num_correct += torch.eq(batch_predict.argmax(dim=1), batch_target).sum()  \n",
    "    val_accuracy = test_total_num_correct.item()/test_size\n",
    "    val_loss = test_loss.item()/test_size\n",
    "    predict = np.concatenate(predict, axis=0)\n",
    "    target = np.concatenate(target, axis=0)\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        number_epoch_until_best_accuracy = epoch_idx\n",
    "        training_time_until_best = training_time\n",
    "        # Save Model\n",
    "        if Scr == 4:\n",
    "            torch.save(model.state_dict(), 'cbsdnn_smote4_nodes.pt')        \n",
    "            state = {\n",
    "            'epoch': NUM_EPOCHS,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            savepath='cbsdnn_smote4_nodes_checkpoint.pth'\n",
    "            torch.save(state,savepath)            \n",
    "        if Scr == 5:\n",
    "            torch.save(model.state_dict(), 'cbsdnn_smote5_nodes.pt')        \n",
    "            state = {\n",
    "            'epoch': NUM_EPOCHS,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            }\n",
    "            savepath='cbsdnn_smote5_nodes_checkpoint.pth'\n",
    "            torch.save(state,savepath)\n",
    "            \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "    print(f'epoch:{epoch_idx}, '\n",
    "          f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "          f'validation loss:{val_loss: .5f}, '\n",
    "          f'accuracy: {val_accuracy: .4f}, '\n",
    "          f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "    if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "        break\n",
    "\n",
    "\n",
    "print(f'total training time: {training_time_until_best}')\n",
    "print(f'number of epochs: {number_epoch_until_best_accuracy}')\n",
    "print(f'time per epoch: {(training_time_until_best/number_epoch_until_best_accuracy): .2f}')    \n",
    "    \n",
    "\n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Loading Model, Further Link Specific Training and Edge Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:\n",
    "    checkpoint = torch.load('cbsdnn_smote4_nodes_checkpoint.pth')\n",
    "if Scr == 5:\n",
    "    checkpoint = torch.load('cbsdnn_smote5_nodes_checkpoint.pth')\n",
    "\n",
    "model1.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs in edge training\n",
    "NUM_EPOCHS_EDGE = 100.\n",
    "prd = []\n",
    "trg = []\n",
    "snr = []\n",
    "\n",
    "# Parameters for kd loss func\n",
    "if Scr == 4:\n",
    "    alpha = 0.4\n",
    "    T = 10.0\n",
    "if Scr == 5:\n",
    "    alpha = 0.5\n",
    "    T = 10.0\n",
    "    \n",
    "for i in range(len(link_data)):\n",
    "    \n",
    "    # Train\n",
    "    edge_train_dataloader=data.DataLoader(data.TensorDataset(edge_train_x[i],edge_train_y[i]),\n",
    "                                     batch_size=TRAIN_BATCH_SIZE, shuffle=True, \n",
    "                                     num_workers=16, pin_memory=True)\n",
    "\n",
    "    edge_val_dataloader=data.DataLoader(data.TensorDataset(edge_val_x[i],edge_val_y[i]),\n",
    "                                   batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                   num_workers=16, pin_memory=True)\n",
    "    start_time = time.time()\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = 100\n",
    "    number_epoch_until_best = 1\n",
    "    training_time = 0\n",
    "    training_time_until_best = 0\n",
    "    average_time_per_epoch = 0\n",
    "    for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "\n",
    "        progress_edge_training_epoch = tqdm(\n",
    "            edge_train_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "            miniters=1, ncols=88, position=0,\n",
    "            leave=True, total=len(edge_train_dataloader), smoothing=.9,disable=True)\n",
    "\n",
    "        progress_edge_validation_epoch = tqdm(\n",
    "            edge_val_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(edge_val_dataloader), smoothing=.9,disable=True)\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_size = 0\n",
    "        model1.eval()\n",
    "        model.train()           \n",
    "        for idx, (sentence, tags) in enumerate(progress_edge_training_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            tag_scores = model(sentence)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = model1(sentence.view(-1,1,3))\n",
    "            loss =loss_kd(tag_scores,tags,teacher_outputs,alpha,T)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * tags.size()[0]\n",
    "            train_size += tags.size()[0]\n",
    "                \n",
    "        test_loss = 0\n",
    "        test_size = 0    \n",
    "        test_total_num_correct = 0\n",
    "        predict = []\n",
    "        target = []\n",
    "        model.eval()\n",
    "        model1.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_validation_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                teacher_outputs = model1(sentence.view(-1,1,3))\n",
    "                loss =loss_kd(tag_scores,tags,teacher_outputs,alpha,T)\n",
    "                predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target.append(tags.cpu().numpy())        \n",
    "                test_loss += loss * tags.size()[0]\n",
    "                test_size += tags.size()[0]\n",
    "                test_total_num_correct += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "        val_accuracy = test_total_num_correct.item()/test_size\n",
    "        val_loss = test_loss.item()/test_size\n",
    "        predict = np.concatenate(predict, axis=0)\n",
    "        target = np.concatenate(target, axis=0)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            number_epoch_until_best_accuracy = epoch_idx\n",
    "            training_time_until_best = training_time\n",
    "            if Scr == 4:\n",
    "                torch.save(model.state_dict(), 'mlp_scr4_kd_nodes_2.pt')\n",
    "            if Scr == 5:\n",
    "                torch.save(model.state_dict(), 'mlp_scr5_kd_nodes_2.pt')\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "        print(f'epoch:{epoch_idx}, '\n",
    "              f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "              f'validation loss:{val_loss: .5f}, '\n",
    "              f'accuracy: {val_accuracy: .4f}, '\n",
    "              f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "        if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "            break\n",
    "\n",
    "    print(f'number of epochs: {number_epoch_until_best_accuracy}')\n",
    "    if Scr == 4:\n",
    "        model.load_state_dict(torch.load('mlp_scr4_kd_nodes_2.pt'))\n",
    "    if Scr == 5:\n",
    "        model.load_state_dict(torch.load('mlp_scr5_kd_nodes_2.pt'))\n",
    "    model.cuda()\n",
    "    \n",
    "    # Test\n",
    "\n",
    "    test_dataloader=data.DataLoader(data.TensorDataset(test_x[i],test_y[i]), \n",
    "                                       batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                                       num_workers=16, pin_memory=True)\n",
    "\n",
    "    progress_test_epoch = tqdm(\n",
    "        test_dataloader, \n",
    "        desc=f'Link {i}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    snrss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "            snrss.append(sentence.cpu().numpy()[:,0])\n",
    "\n",
    "    prd.append(np.concatenate(predict, axis=0))\n",
    "    trg.append(np.concatenate(target, axis=0))\n",
    "    snr.append(np.concatenate(snrss, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr ==4:\n",
    "    mlp_scr4_edge_res = {}\n",
    "\n",
    "    mlp_scr4_edge_res['mlp_scr4_prd'] = prd\n",
    "    mlp_scr4_edge_res['mlp_scr4_trg'] = trg\n",
    "    mlp_scr4_edge_res['mlp_scr4_snr'] = [snrs * scr4_std + scr4_mean for snrs in snr]\n",
    "\n",
    "    outname = 'mlp_scr4_nodes_kd_smote_res.pickle'                 \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr4_edge_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr ==5:\n",
    "    mlp_scr5_edge_res = {}\n",
    "\n",
    "    mlp_scr5_edge_res['mlp_scr5_prd'] = prd\n",
    "    mlp_scr5_edge_res['mlp_scr5_trg'] = trg\n",
    "    mlp_scr5_edge_res['mlp_scr5_snr'] = [snrs * scr5_std + scr5_mean for snrs in snr]\n",
    "\n",
    "    outname = 'mlp_scr5_nodes_kd_smote_res.pickle'                 \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr5_edge_res, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. TF-KD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote data for edge training\n",
    "\n",
    "datadist_smote = [0.8, 0.2] # distribution among train, validation, \n",
    "\n",
    "edge_train_smote_x_npn= []\n",
    "edge_train_smote_y_npn = []\n",
    "edge_val_smote_x_npn = []\n",
    "edge_val_smote_y_npn = []\n",
    "\n",
    "for i in range(len(link_data_smote)):\n",
    "    datalen = len(link_data_smote[i][1])\n",
    "    edge_trainlen_smote = int(datalen*sum(datadist_smote[:1]))\n",
    "    edge_vallen_smote = int(datalen*sum(datadist_smote[:2]))\n",
    "\n",
    "    edge_train_smote_x_npn.append(link_data_smote[i][0][0:edge_trainlen_smote].numpy())    \n",
    "    edge_train_smote_y_npn.append(link_data_smote[i][1][0:edge_trainlen_smote].numpy())\n",
    "    \n",
    "    edge_val_smote_x_npn.append(link_data_smote[i][0][edge_trainlen_smote:edge_vallen_smote].numpy())    \n",
    "    edge_val_smote_y_npn.append(link_data_smote[i][1][edge_trainlen_smote:edge_vallen_smote].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Tensors for Batch Processing\n",
    "# Edge Training and Testing for individual links\n",
    "edge_train_smote_x = [torch.from_numpy(link_x).type(torch.float) for link_x in edge_train_smote_x_npn]\n",
    "edge_val_smote_x = [torch.from_numpy(link_x).type(torch.float) for link_x in edge_val_smote_x_npn]\n",
    "\n",
    "edge_train_smote_y = [torch.from_numpy(link_y).type(torch.long) for link_y in edge_train_smote_y_npn]\n",
    "edge_val_smote_y = [torch.from_numpy(link_y).type(torch.long) for link_y in edge_val_smote_y_npn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 64\n",
    "\n",
    "best_val_accuracy = 0\n",
    "best_val_loss = 100\n",
    "number_epoch_until_best = 1\n",
    "training_time = 0\n",
    "training_time_until_best = 0\n",
    "average_time_per_epoch = 0\n",
    "\n",
    "model = MLP().cuda()\n",
    "model1 = CBSDNN().cuda()\n",
    "loss_function = nn.CrossEntropyLoss(weight=torch.tensor([1.0,1.0])).cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Loading Model, Link Specific Training with Smote and transfer to Scr trainig and Edge Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:\n",
    "    checkpoint = torch.load('cbsdnn_smote4_nodes_checkpoint.pth')\n",
    "if Scr == 5:\n",
    "    checkpoint = torch.load('cbsdnn_smote5_nodes_checkpoint.pth')\n",
    "\n",
    "model1.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs in edge training\n",
    "NUM_EPOCHS_EDGE = 100\n",
    "prd = []\n",
    "trg = []\n",
    "snr = []\n",
    "\n",
    "# Parameters for kd loss func\n",
    "if Scr == 4:\n",
    "    alpha = 0.4\n",
    "    T = 10.0\n",
    "if Scr == 5:\n",
    "    alpha = 0.5\n",
    "    T = 10.0\n",
    "\n",
    "for i in range(len(link_data_smote)):\n",
    "    \n",
    "    # Train\n",
    "    edge_train_smote_dataloader=data.DataLoader(data.TensorDataset(edge_train_smote_x[i],edge_train_smote_y[i]),\n",
    "                                     batch_size=TRAIN_BATCH_SIZE, shuffle=True, \n",
    "                                     num_workers=16, pin_memory=True)\n",
    "\n",
    "    edge_val_smote_dataloader=data.DataLoader(data.TensorDataset(edge_val_smote_x[i],edge_val_smote_y[i]),\n",
    "                                   batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                   num_workers=16, pin_memory=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    best_val_accuracy = 0\n",
    "    best_val_loss = 100\n",
    "    number_epoch_until_best = 1\n",
    "    training_time = 0\n",
    "    training_time_until_best = 0\n",
    "    average_time_per_epoch = 0\n",
    "    for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "\n",
    "        progress_edge_training_smote_epoch = tqdm(\n",
    "            edge_train_smote_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "            miniters=1, ncols=88, position=0,\n",
    "            leave=True, total=len(edge_train_smote_dataloader), smoothing=.9, disable = True)\n",
    "\n",
    "        progress_edge_validation_smote_epoch = tqdm(\n",
    "            edge_val_smote_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(edge_val_smote_dataloader), smoothing=.9, disable = True)\n",
    "        \n",
    "        train_loss = 0\n",
    "        train_size = 0\n",
    "        model1.eval()\n",
    "        model.train()           \n",
    "        for idx, (sentence, tags) in enumerate(progress_edge_training_smote_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            tag_scores = model(sentence)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = model1(sentence.view(-1,1,3))\n",
    "            loss =loss_kd(tag_scores,tags,teacher_outputs,alpha,T)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * tags.size()[0]\n",
    "            train_size += tags.size()[0]\n",
    "                \n",
    "        test_loss = 0\n",
    "        test_size = 0    \n",
    "        test_total_num_correct = 0\n",
    "        predict = []\n",
    "        target = []\n",
    "        model.eval()\n",
    "        model1.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_validation_smote_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                teacher_outputs = model1(sentence.view(-1,1,3))\n",
    "                loss =loss_kd(tag_scores,tags,teacher_outputs,alpha,T)\n",
    "                #loss = loss_function(tag_scores, tags)\n",
    "                predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target.append(tags.cpu().numpy())        \n",
    "                test_loss += loss * tags.size()[0]\n",
    "                test_size += tags.size()[0]\n",
    "                test_total_num_correct += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "        val_accuracy = test_total_num_correct.item()/test_size\n",
    "        val_loss = test_loss.item()/test_size\n",
    "        predict = np.concatenate(predict, axis=0)\n",
    "        target = np.concatenate(target, axis=0)\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            number_epoch_until_best_accuracy = epoch_idx\n",
    "            training_time_until_best = training_time\n",
    "            if Scr == 4:\n",
    "                torch.save(model.state_dict(), 'mlp_smote4_nodes_tf_kd.pt')\n",
    "                # Save Model\n",
    "                state = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),}\n",
    "                savepath='mlp_smote4_nodes_tf_kd.pth'\n",
    "                torch.save(state,savepath)\n",
    "            if Scr == 5:\n",
    "                torch.save(model.state_dict(), 'mlp_smote5_nodes_tf_kd.pt')\n",
    "                # Save Model\n",
    "                state = {\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),}\n",
    "                savepath='mlp_smote5_nodes_tf_kd.pth'\n",
    "                torch.save(state,savepath)\n",
    "                \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            number_epoch_until_best_loss = epoch_idx\n",
    "        \n",
    "        print(f'epoch:{epoch_idx}, '\n",
    "              f'training loss:{train_loss.item()/train_size: .5f}, '\n",
    "              f'validation loss:{val_loss: .5f}, '\n",
    "              f'accuracy: {val_accuracy: .4f}, '\n",
    "              f'best accuracy: {best_val_accuracy: .4f}')\n",
    "\n",
    "        if epoch_idx > number_epoch_until_best_accuracy+4 and epoch_idx > number_epoch_until_best_loss+4:\n",
    "            break\n",
    "\n",
    "    print(f'number of epochs: {number_epoch_until_best_accuracy}')\n",
    "    \n",
    "    # Transfer Learning\n",
    "    if Scr == 4:\n",
    "        checkpoint = torch.load('mlp_smote4_nodes_tf_kd.pth')\n",
    "    if Scr == 5:\n",
    "        checkpoint = torch.load('mlp_smote5_nodes_tf_kd.pth')\n",
    "    \n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.cuda()    \n",
    "    \n",
    "    edge_train_dataloader=data.DataLoader(data.TensorDataset(edge_train_x[i],edge_train_y[i]),\n",
    "                                     batch_size=TRAIN_BATCH_SIZE, shuffle=True, \n",
    "                                     num_workers=16, pin_memory=True)\n",
    "\n",
    "    edge_val_dataloader=data.DataLoader(data.TensorDataset(edge_val_x[i],edge_val_y[i]),\n",
    "                                   batch_size=VAL_BATCH_SIZE, shuffle=False, \n",
    "                                   num_workers=16, pin_memory=True)\n",
    "\n",
    "    \n",
    "    start_time_1 = time.time()\n",
    "    best_val_accuracy_1 = 0\n",
    "    best_val_loss_1 = 100\n",
    "    number_epoch_until_best_1 = 1\n",
    "    training_time_1 = 0\n",
    "    training_time_until_best_1 = 0\n",
    "    average_time_per_epoch_1 = 0\n",
    "    for epoch_idx in range(NUM_EPOCHS_EDGE): \n",
    "        \n",
    "        progress_edge_training_epoch = tqdm(\n",
    "            edge_train_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Training',\n",
    "            miniters=1, ncols=88, position=0,\n",
    "            leave=True, total=len(edge_train_dataloader), smoothing=.9, disable = True)\n",
    "\n",
    "        progress_edge_validation_epoch = tqdm(\n",
    "            edge_val_dataloader, \n",
    "            desc=f'Link {i}, Epoch {epoch_idx+1}/{NUM_EPOCHS}, Validation',\n",
    "            miniters=1, ncols=88, position=0, \n",
    "            leave=True, total=len(edge_val_dataloader), smoothing=.9, disable = True)\n",
    "        \n",
    "        train_loss_1 = 0\n",
    "        train_size_1 = 0\n",
    "        model.train() \n",
    "        for idx, (sentence, tags) in enumerate(progress_edge_training_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            tag_scores = model(sentence)\n",
    "            loss = loss_function(tag_scores, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss_1 += loss * tags.size()[0]\n",
    "            train_size_1 += tags.size()[0]\n",
    "                \n",
    "        test_loss_1 = 0\n",
    "        test_size_1 = 0    \n",
    "        test_total_num_correct_1 = 0\n",
    "        predict_1 = []\n",
    "        target_1 = []\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for idx, (sentence, tags) in enumerate(progress_edge_validation_epoch):\n",
    "                sentence = sentence.cuda()\n",
    "                tags = tags.cuda()\n",
    "                tag_scores = model(sentence)\n",
    "                loss = loss_function(tag_scores, tags)\n",
    "                predict_1.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "                target_1.append(tags.cpu().numpy())        \n",
    "                test_loss_1 += loss * tags.size()[0]\n",
    "                test_size_1 += tags.size()[0]\n",
    "                test_total_num_correct_1 += torch.eq(tag_scores.argmax(dim=1), tags).sum()  \n",
    "        val_accuracy_1 = test_total_num_correct_1.item()/test_size_1\n",
    "        val_loss_1 = test_loss_1.item()/test_size_1\n",
    "        predict_1 = np.concatenate(predict_1, axis=0)\n",
    "        target_1 = np.concatenate(target_1, axis=0)\n",
    "        if val_accuracy_1 > best_val_accuracy_1:\n",
    "            best_val_accuracy_1 = val_accuracy_1\n",
    "            number_epoch_until_best_accuracy_1 = epoch_idx\n",
    "            training_time_until_best_1 = training_time_1\n",
    "            if Scr == 4:\n",
    "                torch.save(model.state_dict(), 'mlp_scr4_nodes_tf_kd_2.pt')\n",
    "            if Scr == 5:\n",
    "                torch.save(model.state_dict(), 'mlp_scr5_nodes_tf_kd_2.pt')\n",
    "                \n",
    "        if val_loss_1 < best_val_loss_1:\n",
    "            best_val_loss_1 = val_loss_1\n",
    "            number_epoch_until_best_loss_1 = epoch_idx\n",
    "        \n",
    "        print(f'epoch:{epoch_idx}, '\n",
    "              f'Main training loss:{train_loss_1.item()/train_size_1: .5f}, '\n",
    "              f'Main validation loss:{val_loss_1: .5f}, '\n",
    "              f'Main accuracy: {val_accuracy_1: .4f}, '\n",
    "              f'Main best accuracy: {best_val_accuracy_1: .4f}')\n",
    "\n",
    "        if epoch_idx > number_epoch_until_best_accuracy_1+4 and epoch_idx > number_epoch_until_best_loss_1+4:\n",
    "            break\n",
    "\n",
    "    print(f'number of epochs: {number_epoch_until_best_accuracy_1}')\n",
    "    if Scr == 4:\n",
    "        model.load_state_dict(torch.load('mlp_scr4_nodes_tf_kd_2.pt'))\n",
    "    if Scr == 5:\n",
    "        model.load_state_dict(torch.load('mlp_scr5_nodes_tf_kd_2.pt'))\n",
    "        \n",
    "    model.cuda()\n",
    "    \n",
    "    # Test\n",
    "\n",
    "    test_dataloader=data.DataLoader(data.TensorDataset(test_x[i],test_y[i]), \n",
    "                                       batch_size=VAL_BATCH_SIZE, shuffle=False,\n",
    "                                       num_workers=16, pin_memory=True)\n",
    "\n",
    "    progress_test_epoch = tqdm(\n",
    "        test_dataloader, \n",
    "        desc=f'Link {i}, Test',\n",
    "        miniters=1, ncols=88, position=0, \n",
    "        leave=True, total=len(test_dataloader), smoothing=.9)\n",
    "\n",
    "    predict = []\n",
    "    target = []\n",
    "    snrss = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (sentence, tags) in enumerate(progress_test_epoch):\n",
    "            sentence = sentence.cuda()\n",
    "            tags = tags.cuda()\n",
    "            tag_scores = model(sentence)\n",
    "            predict.append(tag_scores.argmax(dim=1).cpu().numpy())\n",
    "            target.append(tags.cpu().numpy())\n",
    "            snrss.append(sentence.cpu().numpy()[:,0])\n",
    "\n",
    "    prd.append(np.concatenate(predict, axis=0))\n",
    "    trg.append(np.concatenate(target, axis=0))\n",
    "    snr.append(np.concatenate(snrss, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 4:    \n",
    "    mlp_scr4_edge_res = {}\n",
    "\n",
    "    mlp_scr4_edge_res['mlp_scr4_prd'] = prd\n",
    "    mlp_scr4_edge_res['mlp_scr4_trg'] = trg\n",
    "    mlp_scr4_edge_res['mlp_scr4_snr'] = [snrs * scr4_std + scr4_mean for snrs in snr]\n",
    "\n",
    "\n",
    "    outname = 'mlp_scr4_nodes_tf_kd_res.pickle'        \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr4_edge_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Scr == 5:    \n",
    "    mlp_scr5_edge_res = {}\n",
    "\n",
    "    mlp_scr5_edge_res['mlp_scr5_prd'] = prd\n",
    "    mlp_scr5_edge_res['mlp_scr5_trg'] = trg\n",
    "    mlp_scr5_edge_res['mlp_scr5_snr'] = [snrs * scr5_std + scr5_mean for snrs in snr]\n",
    "\n",
    "\n",
    "    outname = 'mlp_scr5_nodes_tf_kd_res.pickle'        \n",
    "\n",
    "    outfile = os.path.join(os.getcwd(), outname)\n",
    "    if os.path.exists(outfile):\n",
    "            os.replace(outfile, outfile + \".old\")\n",
    "\n",
    "    with open(outfile, 'wb') as file:\n",
    "        pickle.dump(mlp_scr5_edge_res, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
